---
title: "R Simple, Multiple Linear and Stepwise Regression (with Example)"
pagetitle: "Lesson 23"
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  fig.align = "center",
  message = FALSE,
  warning = FALSE
)
```

# Simple linear regression

Linear regression answers a practical question: *is there an approximately linear relationship between a target variable and one (or more) predictors?*

The simplest linear model is:

\[
y = \beta_0 + \beta_1 x + \epsilon
\]

where:

- \(y\) is the dependent (target) variable,
- \(x\) is the independent (predictor) variable,
- \(\epsilon\) is a random error term,
- \(\beta_0\) is the intercept,
- \(\beta_1\) is the slope.

If \(x = 0\), the expected value of \(y\) equals \(\beta_0\). The slope \(\beta_1\) tells how much \(y\) is expected to change for a one-unit increase in \(x\).

To estimate \(\beta_0\) and \(\beta_1\), Ordinary Least Squares (OLS) chooses the line that minimizes the sum of squared residuals (vertical distances between observed and fitted values).

## Scatterplot

We will use a small dataset (Average Heights and Weights for American Women). The dataset has 15 observations, and the goal is to see whether height is positively associated with weight.

```{r}
library(ggplot2)
library(readr)

path <- "raw_data/women.csv"

women_df <- read_csv(path, show_col_types = FALSE)

ggplot(women_df, aes(x = height, y = weight)) +
  geom_point() +
  theme_classic()
```

The scatterplot suggests that weight tends to increase as height increases.

## Least squares estimates (by hand)

In simple linear regression, the slope can be written as:

\[
\hat{\beta}_1 = \frac{\text{Cov}(x,y)}{\text{Var}(x)}
\]

and the intercept as:

\[
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}
\]

You can compute these values in R:

```{r}
beta_1 <- cov(women_df$height, women_df$weight) / var(women_df$height)
beta_1
```

```{r}
beta_0 <- mean(women_df$weight) - beta_1 * mean(women_df$height)
beta_0
```

This manual approach is useful to understand the math, but in practice you should use `lm()`.

## Simple regression with lm()

```{r}
fit_simple <- lm(weight ~ height, data = women_df)
summary(fit_simple)
```

```{r}
coef(fit_simple)
```

# Multiple linear regression

Most real problems involve more than one predictor. A multiple linear regression model is:

\[
y = \beta_0 + \beta_1 x_1 + \cdots + \beta_k x_k + \epsilon
\]

In matrix form:

\[
Y = X\beta + \epsilon
\]

OLS estimates \(\beta\) by minimizing the sum of squared residuals; the closed-form solution is:

\[
\hat{\beta}=(X^TX)^{-1}X^Ty
\]

We will use the `mtcars` dataset and predict fuel efficiency (`mpg`) using continuous predictors.

## Continuous variables only

We start by dropping categorical/binary variables (`am`, `vs`, `cyl`, `gear`, `carb`) and keep only continuous predictors.

```{r}
library(dplyr)

cars_cont <- mtcars |>
  select(-am, -vs, -cyl, -gear, -carb)

glimpse(cars_cont)
```

Fit a multiple regression model:

```{r}
fit_multi <- lm(mpg ~ disp + hp + drat + wt, data = cars_cont)
summary(fit_multi)
```

You can also run an ANOVA table for the fitted model:

```{r}
anova(fit_multi)
```

## Diagnostic plots

```{r}
par(mfrow = c(2, 2))
plot(fit_multi)
par(mfrow = c(1, 1))
```

The `lm()` object also stores useful components:

- `fit_multi$coefficients`
- `fit_multi$residuals`
- `fit_multi$fitted.values`

# Regression with factors

Adding categorical predictors is straightforward, but it is important to ensure they are stored as factors.

```{r}
cars_all <- mtcars |>
  mutate(
    across(c(cyl, vs, am, gear, carb), as.factor)
  )

fit_factors <- lm(mpg ~ ., data = cars_all)
summary(fit_factors)
```

R uses the first factor level as the reference group, and the other coefficients represent differences relative to that baseline.

# Stepwise regression (model selection)

Stepwise regression is an automated procedure that adds/removes predictors to optimize a criterion (often AIC). In base R, this is done with `stats::step()` which uses AIC by default. [web:554]

This approach is popular for quick exploration, but it can be unstable and should be used cautiously; it is mainly included here for illustration.

## Correlation overview (optional)

```{r}
library(GGally)

ggscatmat(cars_cont, columns = 1:ncol(cars_cont))
```

## Stepwise selection with base R (AIC)

We define a “full” model and a “null” (intercept-only) model, then run stepwise search.

```{r}
fit_full <- lm(mpg ~ ., data = cars_cont)
fit_null <- lm(mpg ~ 1, data = cars_cont)

fit_step_aic <- step(
  fit_null,
  scope = list(lower = formula(fit_null), upper = formula(fit_full)),
  direction = "both",
  trace = 0
)

summary(fit_step_aic)
```

## Stepwise / all-subsets with olsrr (optional)

The `olsrr` package can fit many subsets and compare them. For example, `ols_step_all_possible()` fits all combinations up to a chosen order. [web:546]

```{r, eval=FALSE}
library(olsrr)

test_all <- ols_step_all_possible(fit_full)
plot(test_all)

# Example (both-direction stepwise based on p-values)
ols_step_both_p(fit_full, pent = 0.1, prem = 0.3, details = FALSE)
```

# Summary

```{r, echo=FALSE}
knitr::kable(
  data.frame(
    Topic = c("Simple linear regression", "Multiple regression", "Factors in regression", "Stepwise (AIC)"),
    Main_function = c("lm()", "lm()", "lm()", "stats::step()"),
    Notes = c(
      "Understand slope/intercept; use lm() in practice",
      "Multiple predictors; check diagnostics",
      "Convert categorical variables with as.factor()",
      "Automated model selection using AIC"
    )
  )
)
```
