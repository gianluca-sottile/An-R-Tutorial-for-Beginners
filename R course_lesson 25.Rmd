---
title: "K-means Clustering in R (with Example)"
pagetitle: "Lesson 25"
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  fig.align = "center",
  message = FALSE,
  warning = FALSE
)
```

# What is cluster analysis?

Cluster analysis is a core technique in **unsupervised learning**. A *cluster* is a group of observations that are similar to one another according to a chosen distance measure. In many business contexts, clustering is more about *discovery* than prediction: the goal is to find structure in the data and then interpret it.

Typical applications include:

- Customer segmentation: Identify groups of customers with similar behavior.
- Stock clustering: Group stocks that move similarly over time.
- Pattern discovery: Find groups of observations that share common feature profiles.

A key distinction from supervised learning is that clustering creates a new variable (a cluster label), rather than predicting a known target.

To build intuition, consider a simple dataset with customer age and total spend:

```{r}
library(ggplot2)

toy <- data.frame(
  age   = c(18, 21, 22, 24, 26, 26, 27, 30, 31, 35, 39, 40, 41, 42, 44, 46, 47, 48, 49, 54),
  spend = c(10, 11, 22, 15, 12, 13, 14, 33, 39, 37, 44, 27, 29, 20, 28, 21, 30, 31, 23, 24)
)

ggplot(toy, aes(age, spend)) +
  geom_point() +
  labs(x = "Age", y = "Total spend")
```

Visually, you may already guess a few groups. K-means is a simple and popular method to create those groups in a reproducible way.

# The k-means algorithm

K-means is one of the most widely used clustering algorithms. Given a number of clusters \(k\), it partitions the data by minimizing the within-cluster variability (points in the same cluster should be close to each other).

A simplified workflow is:

- Initialize \(k\) centroids (often randomly).
- Assign each observation to the nearest centroid (usually Euclidean distance).
- Update each centroid as the mean of the assigned observations.
- Repeat until assignments stop changing (or a maximum number of iterations is reached).

Because the initialization is random, k-means can converge to different local optima. For this reason, it is standard practice to use multiple random starts with `nstart` (e.g., 25 or 50) and keep the best solution.

# Import data

We will use the “Prices of Personal Computers” dataset (1993–1995). K-means relies on distances, so categorical variables are not directly appropriate in this form; we remove a few categorical columns and keep the numeric ones.

```{r}
library(dplyr)
library(readr)

path <- "https://raw.githubusercontent.com/guru99-edu/R-Programming/master/computers.csv"

df <- read_csv(path, show_col_types = FALSE) |>
  select(-cd, -multi, -premium)

glimpse(df)
```

# Standardize features

Distance-based methods are sensitive to variable scales (e.g., `price` vs `ram`). Standardization (z-scores) is therefore a common pre-processing step for k-means.

```{r}
summary(df)
```

```{r}
rescale_df <- df |>
  mutate(across(where(is.numeric), scale)) |>
  as.data.frame()
```

# Train a first k-means model

```{r}
set.seed(2345)
pc_cluster <- kmeans(rescale_df, centers = 5, nstart = 25)
pc_cluster$size
```

Key objects returned by `kmeans()`:

- `$cluster`: the assigned cluster for each observation.
- `$centers`: the cluster centroids.
- `$tot.withinss`: total within-cluster sum of squares (WSS).
- `$betweenss`: between-cluster sum of squares.
- `$size`: cluster sizes.

# Choosing the number of clusters (k)

There is no universally correct \(k\). In practice, common approaches include:

- Elbow method (WSS).
- Average silhouette method.
- Gap statistic.

## Elbow method (WSS)

Compute WSS over a range of \(k\) values and look for a “knee” (diminishing returns).

```{r}
kmeans_wss <- function(k, x, seed = 2345, nstart = 25) {
  set.seed(seed)
  kmeans(x, centers = k, nstart = nstart)$tot.withinss
}

max_k <- 20
elbow <- data.frame(
  k   = 2:max_k,
  wss = sapply(2:max_k, kmeans_wss, x = rescale_df)
)

ggplot(elbow, aes(k, wss)) +
  geom_point() +
  geom_line() +
  scale_x_continuous(breaks = 2:max_k) +
  labs(x = "Number of clusters (k)", y = "Total within-cluster sum of squares (WSS)")
```

## Average silhouette method (optional)

Silhouette width measures how well-separated clusters are; higher average silhouette is generally better.

```{r, eval=FALSE}
library(factoextra)

set.seed(2345)
fviz_nbclust(rescale_df, kmeans, method = "silhouette", k.max = 20, nstart = 25) +
  labs(subtitle = "Average silhouette method")
```

## Gap statistic (optional)

The gap statistic compares the observed clustering structure to a reference null distribution to help choose \(k\).

```{r, eval=FALSE}
library(cluster)
library(factoextra)

set.seed(2345)
gap_stat <- clusGap(rescale_df, FUN = kmeans, K.max = 20, B = 50, nstart = 25)

fviz_gap_stat(gap_stat)
```

In the rest of this lesson, we continue with \(k = 7\) (as an example) and show how to interpret and validate the solution.

# Fit the final model (k = 7)

```{r}
set.seed(2345)
pc_cluster_2 <- kmeans(rescale_df, centers = 7, nstart = 25)

pc_cluster_2$size
```

## Examine cluster centers

Because the features are standardized, each centroid coordinate is a z-score: positive values are above the overall mean for that feature, and negative values are below it.

```{r}
centers <- pc_cluster_2$centers
centers
```

# Cluster profiles (heatmap)

A heatmap makes it easier to compare centroid profiles across clusters.

```{r}
library(tidyr)

centers_df <- data.frame(
  cluster = factor(seq_len(nrow(centers))),
  centers,
  row.names = NULL
)

centers_long <- centers_df |>
  pivot_longer(
    cols = -cluster,
    names_to = "feature",
    values_to = "value"
  )
```

```{r}
library(RColorBrewer)

hm_palette <- colorRampPalette(rev(brewer.pal(10, "RdYlGn")), space = "Lab")

ggplot(centers_long, aes(x = feature, y = cluster, fill = value)) +
  geom_tile() +
  scale_fill_gradientn(colours = hm_palette(90)) +
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

# Interpretation and validation

## Practical interpretation

At this point, the goal is to map each cluster to a meaningful description (e.g., “high price + high RAM” machines vs “budget machines”). Standardization helps because every feature is on the same scale.

A useful quick check is:
- Are cluster sizes reasonable (no tiny clusters unless expected)?
- Do centroid profiles differ in meaningful ways?

## Internal validation with silhouette

Silhouette values summarize cohesion and separation; the average silhouette width can be used as an internal quality check.

```{r}
library(cluster)

d <- dist(rescale_df)  # Euclidean distance matrix
sil <- silhouette(pc_cluster_2$cluster, d)

summary(sil)
plot(sil, border = NA)
```

## Simple stability check (sensitivity to random starts)

K-means can vary depending on initialization. A simple diagnostic is to re-fit the model multiple times (same k) and examine how much the objective function varies.

```{r}
set.seed(2345)

k <- 7
n_rep <- 20

wss_rep <- replicate(
  n_rep,
  kmeans(rescale_df, centers = k, nstart = 25)$tot.withinss
)

summary(wss_rep)
```

If the objective varies substantially, consider increasing `nstart` (e.g., 50), revisiting preprocessing (outliers, scaling), or trying alternative clustering methods (e.g., PAM, hierarchical clustering).

# Summary

- K-means is a simple and popular clustering algorithm for numeric data.
- Standardizing features is important because k-means is distance-based.
- Choosing \(k\) can be guided by elbow (WSS), silhouette, and gap statistic.
- Use `nstart` (e.g., 25+) and a fixed seed for more stable results.

```{r, echo=FALSE}
library(knitr)

dt <- data.frame(
  Library   = c("stats (base R)", "cluster", "factoextra"),
  Objective = c("Fit k-means", "Silhouette & gap statistic", "Visualize optimal k"),
  Function  = c("kmeans()", "silhouette(), clusGap()", "fviz_nbclust(), fviz_gap_stat()"),
  Notes     = c("Use nstart for stability", "Distance-based validation", "Optional but convenient")
)

knitr::kable(dt)
```
