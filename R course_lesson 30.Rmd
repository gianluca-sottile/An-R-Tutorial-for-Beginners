---
pagetitle: "Lesson 30"
title: "Principal Component Analysis (PCA) in R — Theory & Practice with prcomp()"
editor_options:
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  fig.align = "center",
  message = FALSE,
  warning = FALSE
)
```

# Principal Component Analysis (PCA)

Principal Component Analysis (PCA) finds **orthogonal linear combinations** of $p$ standardized variables $Z = (Z_1, \dots, Z_p)^T$ maximizing successive variances:

$$
\text{PC}_k = v_k^T Z, \quad v_k^T \Sigma v_k \to \max, \quad v_k \perp v_1, \dots, v_{k-1}
$$

Solved via **eigen/SVD decomposition** $\Sigma = V \Lambda V^T$, where $V$ = **loadings** (rotation matrix), $\Lambda$ = eigenvalues ($\text{Var}(\text{PC}_k) = \lambda_k$). Scores = $ZV_k$. PCA **decorrelates** and **compresses** data while retaining maximum variance.

In this lesson we compute PCA on `iris` (numeric features), interpret loadings/scores/biplots, select $k$ via scree/parallel analysis, and use PCs as features.

# Step 1: Data preparation

```{r}
library(dplyr)

data("iris")
iris_num <- iris |>
  select(where(is.numeric))  # Sepal/Petal: Length/Width

glimpse(iris_num)
summary(iris_num)
```

**Critical**: Center ($Z_j = (X_j - \bar{X}_j)/s_j$) and scale ($s_j = \text{SD}$) to equalize variable contributions.

# Step 2: PCA computation

```{r}
pca_iris <- prcomp(
  x = iris_num,
  center = TRUE,
  scale. = TRUE,
  retx = TRUE
)

pca_iris
```

**`prcomp` structure**:  
- `sdev`: $\sqrt{\lambda_k}$ (PC standard deviations).  
- `rotation`: $V$ (**loadings**/eigenvectors).  
- `x`: **Scores** $ZV$.  
- `center`, `scale`: Transformations applied.

# Step 3: Variance decomposition & screeplot

```{r}
pca_summary <- summary(pca_iris)
print(pca_summary, loadings = FALSE, digits = 3)
```

**Outputs**:  
- **Proportion**: $\lambda_k / \sum \lambda_k$.  
- **Cumulative**: Sufficient PCs for 80–95% variance (here PC1+PC2 = 95.8%).

```{r}
library(ggplot2)
scree_df <- data.frame(
  PC = factor(1:4),
  Variance = (pca_iris$sdev)^2,
  PropVar = pca_summary$importance[2, ]
)

ggplot(scree_df, aes(x = PC, y = Variance)) +
  geom_col(fill = "steelblue", alpha = 0.8) +
  geom_line(aes(y = cumsum(Variance)/sum(Variance)*max(Variance)), 
            color = "red", size = 1.2) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "darkred") +
  labs(
    title = "Screeplot & Cumulative Variance — Iris PCA",
    subtitle = "Kaiser criterion: λ > 1 | PC1+PC2 = 95.8%",
    x = "Principal Component",
    y = "Eigenvalue (Variance)"
  ) +
  theme_minimal()
```

**Kaiser criterion**: Retain $\lambda_k > 1$ (PC1–PC3 here).

# Step 4: Loadings matrix (rotation)

```{r}
pca_iris$rotation
round(pca_iris$rotation[, 1:2], 3)
```

**PC1** ($73\%$): Positive on all → **overall size** (Sepal/Petal lengths dominate).  
**PC2** ($23\%$): Petal Width vs Length → **shape**.  
**Sign ambiguity**: Flip $v_k \to -v_k$ equivalent (focus on $|v_{jk}|$, pattern).

# Step 5: Scores & species separation

```{r}
scores_df <- data.frame(
  pca_iris$x,
  Species = iris$Species
)

ggplot(scores_df, aes(x = PC1, y = PC2, color = Species)) +
  geom_point(size = 3, alpha = 0.8) +
  labs(
    title = "PCA Scores Plot — Iris",
    subtitle = "PC1+PC2 explain 95.8% | setosa perfectly separated",
    x = "PC1 (73.0%)",
    y = "PC2 (22.8%)"
  ) +
  theme_minimal()
```

**setosa** linearly separable; versicolor/virginica overlap slightly.

# Step 6: Biplot (scores + loadings)

```{r}
biplot(pca_iris, 
       choices = c(1, 2), 
       scale = 0.5,         # balance arrows vs points
       main = "PCA Biplot — Iris")
```

**Arrows**: Variable correlations ($\cos \theta_{jk} = v_{jk}$). Petal variables aligned → PC1; Width opposes Length → PC2.

# Step 7: PCs as features (downstream modeling)

```{r}
library(caret)
set.seed(123)

pc2_df <- data.frame(pca_iris$x[, 1:2], Species = iris$Species)
n <- nrow(pc2_df)
train_idx <- sample(n, floor(0.7 * n))

knn_pca <- train(
  Species ~ ., 
  data = pc2_df[train_idx, ],
  method = "knn",
  trControl = trainControl(method = "cv", number = 5),
  tuneLength = 5
)

pred_pca <- predict(knn_pca, pc2_df[-train_idx, -3])
confusionMatrix(pred_pca, pc2_df$Species[-train_idx])
```

**95% accuracy** with 2 PCs vs 4 originals (dim. reduction + multicollinearity fix).

# Best practices & warnings

- **Always scale** ($Z$-scores) unless units comparable.  
- **Interpret loadings pattern**, not absolute signs.  
- **Cumulative variance 80–95%** typical cutoff.  
- **Not rotation invariant**: Different software may flip signs/order.  
- **Assumes linearity**: Use kernel PCA/t-SNE for manifolds.

# Summary

You learned PCA with `prcomp()` to:

- Compute **SVD-based decorrelation**: $\Sigma = V\Lambda V^T$.  
- Select $k$ via scree/parallel/Kaiser ($\lambda_k > 1$).  
- Interpret **loadings** ($v_{jk}$), **scores** ($ZV$), biplots.  
- Extract features for modeling (95% accuracy here).  

PCA is foundational for **unsupervised dim. reduction** and preprocessing.

